{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<center><h1>Diving into Data Preprocessing</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Table of contents\n",
    "\n",
    "* [Feature Engineering](#feat_engg)\n",
    "    * [Missing Values Treatment](#missing)\n",
    "    * [Outliers Treatment](#outliers)\n",
    "    * [Categorical Data Handling](#cat)\n",
    "    * [Imbalanced Class Handling](#imbal)\n",
    "    * [Data Transformation](#trans)\n",
    "    * [Extracting Date](#date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<a id='feat_engg'></a>\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### What is Feature Engineering?\n",
    "**Feature engineering is about creating new input features from your existing ones.**\n",
    "\n",
    "This is often one of the most valuable tasks a data scientist can do to improve model performance, for 3 big reasons:\n",
    "\n",
    "* You can isolate and highlight key information, which helps your algorithms \"focus\" on what’s important.\n",
    "* You can bring in your own domain expertise.\n",
    "* Most importantly, once you understand the \"vocabulary\" of feature engineering, you can bring in other people’s domain expertise!\n",
    "\n",
    "*\"The features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering.\"* — Luca Massaron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dt = {\"col1\":[51,22,13,64,50,np.nan,17,580,19,1000],\n",
    "      \"col2\":[np.nan,np.nan,89,np.nan,76,np.nan,53,np.nan,900,np.nan],\n",
    "      \"col3\":['male','male','male','male','male','male','male',np.nan,'female',np.nan],\n",
    "      \"col4\":['good','bad','good',np.nan,'good','bad','bad',np.nan,'good',np.nan]}\n",
    "\n",
    "data = pd.DataFrame(dt)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<a id='missing'></a>\n",
    "### Missing Values Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Analyze missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "# Dropping columns with missing value rate higher than threshold\n",
    "data[data.columns[data.isnull().mean() < threshold]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Dropping rows with missing value rate higher than threshold\n",
    "data.loc[data.isnull().mean(axis=1) < threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Impute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Numerical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "num_cols = ['col1', 'col2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filling all missing values with 0\n",
    "data[num_cols].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filling missing values with medians of the columns\n",
    "data['col1'].fillna(data['col1'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Fill all numerical columns\n",
    "for col in num_cols:\n",
    "    data[col] = data[col].fillna(data[col].median())\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Categorical Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "cat_cols = ['col3', 'col4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Max fill function for categorical columns\n",
    "data['col3'].fillna(data['col3'].value_counts().idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "# Fill all categorical columns\n",
    "for col in cat_cols:\n",
    "    data[col] = data[col].fillna(data[col].value_counts().idxmax())\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "<a id='outliers'></a>\n",
    "### Outliers Treatment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Detect outliers using boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.plot.box();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "#### Detect outliers using interquartile range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def detect_outlier(feature):\n",
    "    Q1 = feature.quantile(0.25)\n",
    "    Q3 = feature.quantile(0.75)\n",
    "    \n",
    "    IQR = Q3-Q1\n",
    "    \n",
    "    lower_bound = Q1-(1.5*IQR)\n",
    "    upper_bound = Q3+(1.5*IQR)\n",
    "    \n",
    "    return feature.index[(feature<lower_bound)|(feature>upper_bound)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    print(col,'-->',detect_outlier(data[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Caping the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    indx = detect_outlier(data[col])\n",
    "    data[col].loc[indx] = data[col].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Droping the rows that contain outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    indx = detect_outlier(data[col])\n",
    "    data[col].loc[indx] = np.nan\n",
    "\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cat'></a>\n",
    "### Categorical Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in cat_cols:\n",
    "    data[col] = data[col].astype('category')\n",
    "    print(col,'---->', dict(enumerate(data[col].cat.categories)))\n",
    "    data[col] = data[col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=cat_cols, prefix=cat_cols)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='imbal'></a>\n",
    "### Imbalanced Class Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.datasets import make_imbalance\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "X, y = make_imbalance(X, y, sampling_strategy={0: 10, 1: 20, 2: 30}, random_state=42)\n",
    "\n",
    "print(\"Data shape:\", y.shape)\n",
    "pd.Series(y).value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oversampling: SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "X_smt, y_smt = sm.fit_resample(X, y)\n",
    "\n",
    "print(\"Data shape:\", y_smt.shape)\n",
    "pd.Series(y_smt).value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_rus, y_rus = rus.fit_resample(X, y)\n",
    "\n",
    "print(\"Data shape:\", y_rus.shape)\n",
    "pd.Series(y_rus).value_counts().plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='trans'></a>\n",
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation is performed to standardize the range of features of data. Since, the range of values of data may vary widely, it becomes a necessary step in data preprocessing while using machine learning algorithms.\n",
    "\n",
    "There are 3 popular methods to transform data:\n",
    "* Scaling\n",
    "* Normalization\n",
    "* Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "In scaling, you transform the data such that the features are within a specific range e.g. [0, 1].\n",
    "\n",
    "${\\displaystyle x'={\\frac {x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}}}$\n",
    "\n",
    "where ${\\displaystyle x}$ is an original value, ${\\displaystyle x'}$ is the rescaled value. \n",
    "\n",
    "Scaling is important in the algorthms such as support vector machines (SVM) and k-nearest neighbors\n",
    "(KNN) where distance betYouen the data points is important. For example, in the dataset containing\n",
    "prices of products; without scaling, SVM might treat 1 USD equivalent to 1 INR though 1 USD = 65\n",
    "INR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# generate 1000 data points randomly drawn from an exponential distr\n",
    "original_data = np.random.exponential(size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mix-max scale the data betYouen 0 and 1\n",
    "scaled_data = minmax_scale(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "Normal distribution (Gaussian distribution), also known as the bell curve, is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean.\n",
    "\n",
    "The general formula is given as:\n",
    "\n",
    "${\\displaystyle x'={\\frac {x-{\\text{mean}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}}}$\n",
    "\n",
    "where ${\\displaystyle x}$ is an original value, ${\\displaystyle x'}$ is the normalized value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Originabl Data\")\n",
    "\n",
    "sns.distplot(normalized_data[0], ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scaling, you’re changing the range of your data while in normalization you’re changing the shape of\n",
    "the distribution of your data.\n",
    "\n",
    "You need to normalize our data if you’re going use a machine learning or statistics technique that\n",
    "assumes that data is normally distributed e.g. t-tests, ANOVAs, linear regression, linear discriminant\n",
    "analysis (LDA) and Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "Standardization transforms your data such that the resulting distribution has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.\n",
    "\n",
    "${\\displaystyle x'={\\frac {x-{\\bar {x}}}{\\sigma }}}$\n",
    "\n",
    "Where $x$ is the original feature vector, ${\\bar{x}={\\text{average}}(x)}$ is the mean of that feature vector, and $\\sigma$ is its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standerdized_data = StandardScaler().fit_transform(original_data.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Originabl Data\")\n",
    "\n",
    "sns.distplot(standerdized_data, ax=ax[1])\n",
    "ax[1].set_title(\"Standerdized data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s widely used in SVMs, logistics regression and neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications of Data Transformation\n",
    "In stochastic gradient descent, feature scaling can sometimes improve the convergence speed of the algorithm. In support vector machines, it can reduce the time to find support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='date'></a>\n",
    "### Extracting Date\n",
    "We can perform the following engineering to the date time variales:\n",
    "* Extracting the parts of the date into different columns: Year, month, day, etc.\n",
    "* Extracting the time period between the current date and columns in terms of years, months, days, etc.\n",
    "* Extracting some specific features from the date: Name of the weekday, Weekend or not, holiday or not, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = pd.DataFrame({'date':['01-01-2017', \n",
    "                             '04-12-2008', \n",
    "                             '23-06-1988', \n",
    "                             '25-08-1999', \n",
    "                             '20-02-1993',]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform string to date\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Year\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting Month\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting passed years since the date\n",
    "data['passed_years'] = date.today().year - data['date'].dt.year\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting passed months since the date\n",
    "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the weekday name of the date\n",
    "data['day_name'] = data['date'].dt.day_name()\n",
    "\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
